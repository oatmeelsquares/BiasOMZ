{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstartion of Reject Option Classification with AIF360\n",
    "- The first part is to show the AIF360 Reject Option Classification algorithm\n",
    "- The second part is to show the AIF360 ROC algorithm with FairFace dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: AIF360 Reject Option Classification (AIF360 Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of the Reject Option Classification (ROC) post-processing algorithm for bias mitigation.\n",
    "- The debiasing function used is implemented in the `RejectOptionClassification` class.\n",
    "- Divide the dataset into training, validation, and testing partitions.\n",
    "- Train classifier on original training data.\n",
    "- Estimate the optimal classification threshold, that maximizes balanced accuracy without fairness constraints.\n",
    "- Estimate the optimal classification threshold, and the critical region boundary (ROC margin) using a validation set for the desired constraint on fairness. The best parameters are those that maximize the classification threshold while satisfying the fairness constraints.\n",
    "- The constraints can be used on the following fairness measures:\n",
    "    * Statistical parity difference on the predictions of the classifier\n",
    "    * Average odds difference for the classifier\n",
    "    * Equal opportunity difference for the classifier\n",
    "- Determine the prediction scores for testing data. Using the estimated optimal classification threshold, compute accuracy and fairness metrics.\n",
    "- Using the determined optimal classification threshold and the ROC margin, adjust the predictions. Report accuracy and fairness metric on the new predictions.\n",
    "\n",
    "source: https://github.com/Trusted-AI/AIF360/blob/main/examples/demo_reject_option_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/Trusted-AI/AIF360\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install aif360\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install common-utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from warnings import warn\n",
    "\n",
    "\n",
    "\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas\n",
    "from aif360.algorithms.postprocessing.reject_option_classification\\\n",
    "        import RejectOptionClassification\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "from aif360.detectors.mdss.generator import get_random_subset\n",
    "\n",
    "from aif360.sklearn.datasets import fetch_adult\n",
    "#from common_utils import compute_metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interactive, FloatSlider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install aif360[Reductions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "from collections import OrderedDict\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "\n",
    "def compute_metrics(dataset_true, dataset_pred,\n",
    "                    unprivileged_groups, privileged_groups,\n",
    "                    disp=True):\n",
    "    \"\"\" Compute the key metrics \"\"\"\n",
    "    classified_metric_pred = ClassificationMetric(dataset_true,\n",
    "                                                  dataset_pred,\n",
    "                                                  unprivileged_groups=unprivileged_groups,\n",
    "                                                  privileged_groups=privileged_groups)\n",
    "    metrics = OrderedDict()\n",
    "    metrics[\"Balanced accuracy\"] = 0.5*(classified_metric_pred.true_positive_rate() +\n",
    "                                        classified_metric_pred.true_negative_rate())\n",
    "    metrics[\"Statistical parity difference\"] = classified_metric_pred.statistical_parity_difference()\n",
    "    metrics[\"Disparate impact\"] = classified_metric_pred.disparate_impact()\n",
    "    metrics[\"Average odds difference\"] = classified_metric_pred.average_odds_difference()\n",
    "    metrics[\"Equal opportunity difference\"] = classified_metric_pred.equal_opportunity_difference()\n",
    "    metrics[\"Theil index\"] = classified_metric_pred.theil_index()\n",
    "\n",
    "    if disp:\n",
    "        for k in metrics:\n",
    "            print(\"%s = %.4f\" % (k, metrics[k]))\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded adult.data to c:\\Users\\n_oha\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\aif360\\data\\raw\\adult\\adult.data\n",
      "Downloaded adult.test to c:\\Users\\n_oha\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\aif360\\data\\raw\\adult\\adult.test\n",
      "Downloaded adult.names to c:\\Users\\n_oha\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\aif360\\data\\raw\\adult\\adult.names\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "urls = {\n",
    "    \"adult.data\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    \"adult.test\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "    \"adult.names\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\"\n",
    "}\n",
    "\n",
    "# Replace the path with your own path to aif360 direcotry\n",
    "# my_data_folder = os.path.join(\n",
    "#     r\"c:\\Users\\your_directory\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\aif360\\data\\raw\\adult\")\n",
    "\n",
    "for filename, url in urls.items():\n",
    "    file_path = os.path.join(my_data_folder, filename)\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(f\"Downloaded {filename} to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "dataset_used = \"adult\"  # \"adult\", \"german\", \"compas\"\n",
    "protected_attribute_used = 1  # 1, 2\n",
    "\n",
    "if dataset_used == \"adult\":\n",
    "    #     dataset_orig = AdultDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        dataset_orig = load_preproc_data_adult(['sex'])\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "        dataset_orig = load_preproc_data_adult(['race'])\n",
    "\n",
    "elif dataset_used == \"german\":\n",
    "    #     dataset_orig = GermanDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        dataset_orig = load_preproc_data_german(['sex'])\n",
    "    else:\n",
    "        privileged_groups = [{'age': 1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        dataset_orig = load_preproc_data_german(['age'])\n",
    "\n",
    "elif dataset_used == \"compas\":\n",
    "    #     dataset_orig = CompasDataset()\n",
    "    if protected_attribute_used == 1:\n",
    "        privileged_groups = [{'sex': 1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        dataset_orig = load_preproc_data_compas(['sex'])\n",
    "    else:\n",
    "        privileged_groups = [{'race': 1}]\n",
    "        unprivileged_groups = [{'race': 0}]\n",
    "        dataset_orig = load_preproc_data_compas(['race'])\n",
    "\n",
    "\n",
    "# Metric used (should be one of allowed_metrics)\n",
    "metric_name = \"Statistical parity difference\"\n",
    "\n",
    "# Upper and lower bound on the fairness metric used\n",
    "metric_ub = 0.05\n",
    "metric_lb = -0.05\n",
    "\n",
    "# random seed for calibrated equal odds prediction\n",
    "np.random.seed(1)\n",
    "\n",
    "# Verify metric name\n",
    "allowed_metrics = [\"Statistical parity difference\",\n",
    "                   \"Average odds difference\",\n",
    "                   \"Equal opportunity difference\"]\n",
    "if metric_name not in allowed_metrics:\n",
    "    raise ValueError(\"Metric name should be one of allowed metrics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the dataset and split into train and test\n",
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split(\n",
    "    [0.5], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up training data and display properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34189, 18)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes,\n",
    "      dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for original training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.190244\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train,\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" %\n",
    "      metric_orig_train.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic regression classifier and predictions\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "y_train_pred = lmod.predict(X_train)\n",
    "\n",
    "# positive class index\n",
    "pos_ind = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_train_pred.labels = y_train_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain scores for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "X_valid = scale_orig.transform(dataset_orig_valid_pred.features)\n",
    "y_valid = dataset_orig_valid_pred.labels\n",
    "dataset_orig_valid_pred.scores = lmod.predict_proba(\n",
    "    X_valid)[:, pos_ind].reshape(-1, 1)\n",
    "\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "dataset_orig_test_pred.scores = lmod.predict_proba(\n",
    "    X_test)[:, pos_ind].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal parameters from the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best threshold for classification only (no fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.7463\n",
      "Optimal classification threshold (no fairness constraints) = 0.2872\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 100\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "\n",
    "    fav_inds = dataset_orig_valid_pred.scores > class_thresh\n",
    "    dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "    dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "\n",
    "    classified_metric_orig_valid = ClassificationMetric(dataset_orig_valid,\n",
    "                                                        dataset_orig_valid_pred,\n",
    "                                                        unprivileged_groups=unprivileged_groups,\n",
    "                                                        privileged_groups=privileged_groups)\n",
    "\n",
    "    ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\n",
    "                       + classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" %\n",
    "      best_class_thresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate optimal parameters for the ROC method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups,\n",
    "                                 privileged_groups=privileged_groups,\n",
    "                                 low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                 num_class_thresh=100, num_ROC_margin=50,\n",
    "                                 metric_name=metric_name,\n",
    "                                 metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "ROC = ROC.fit(dataset_orig_valid, dataset_orig_valid_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.1981\n",
      "Optimal ROC margin = 0.1011\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal classification threshold (with fairness constraints) = %.4f\" %\n",
    "      ROC.classification_threshold)\n",
    "print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7463\n",
      "Statistical parity difference = -0.3670\n",
      "Disparate impact = 0.2744\n",
      "Average odds difference = -0.3140\n",
      "Equal opportunity difference = -0.3666\n",
      "Theil index = 0.1113\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_valid_pred.scores > best_class_thresh\n",
    "dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\n",
    "    \"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_valid_bef = compute_metrics(dataset_orig_valid, dataset_orig_valid_pred,\n",
    "                                   unprivileged_groups, privileged_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Validation set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7090\n",
      "Statistical parity difference = -0.0454\n",
      "Disparate impact = 0.8996\n",
      "Average odds difference = 0.0363\n",
      "Equal opportunity difference = 0.0197\n",
      "Theil index = 0.1172\n"
     ]
    }
   ],
   "source": [
    "# Transform the validation set\n",
    "dataset_transf_valid_pred = ROC.predict(dataset_orig_valid_pred)\n",
    "\n",
    "display(Markdown(\"#### Validation set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_valid_aft = compute_metrics(dataset_orig_valid, dataset_transf_valid_pred,\n",
    "                                   unprivileged_groups, privileged_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7437\n",
      "Statistical parity difference = -0.3580\n",
      "Disparate impact = 0.2794\n",
      "Average odds difference = -0.3181\n",
      "Equal opportunity difference = -0.3769\n",
      "Theil index = 0.1129\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the test set\n",
    "fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\n",
    "    \"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred,\n",
    "                                  unprivileged_groups, privileged_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7141\n",
      "Statistical parity difference = -0.0402\n",
      "Disparate impact = 0.9088\n",
      "Average odds difference = 0.0423\n",
      "Equal opportunity difference = 0.0407\n",
      "Theil index = 0.1171\n"
     ]
    }
   ],
   "source": [
    "# Metrics for the transformed test set\n",
    "dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "display(Markdown(\"#### Test set\"))\n",
    "display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred,\n",
    "                                  unprivileged_groups, privileged_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Optimal Parameters\n",
    "We show the optimal parameters for all combinations of metrics optimized, datasets, and protected attributes below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metric: Statistical parity difference, Accuracy Metric: Balanced accuracy\n",
    "\n",
    "#### Performance\n",
    "\n",
    "| Dataset |Sex (Acc-Bef)|Sex (Acc-Aft)|Sex (Fair-Bef)|Sex (Fair-Aft)|Race/Age (Acc-Bef)|Race/Age (Acc-Aft)|Race/Age (Fair-Bef)|Race/Age (Fair-Aft)|\n",
    "|-|-|-|-|-|-|-|-|-|\n",
    "|Adult (Valid)|0.7473|0.6051|-0.3703|-0.0436|0.7473|0.6198|-0.2226|-0.0007|\n",
    "|Adult (Test)|0.7417|0.5968|-0.3576|-0.0340|0.7417|0.6202|-0.2279|0.0006|\n",
    "|German (Valid)|0.6930|0.6991|-0.0613|0.0429|0.6930|0.6607|-0.2525|-0.0328|\n",
    "|German (Test)|0.6524|0.6460|-0.0025|0.0410|0.6524|0.6317|-0.3231|-0.1038|\n",
    "|Compas (Valid)|0.6599|0.6400|-0.2802|0.0234|0.6599|0.6646|-0.3225|-0.0471|\n",
    "|Compas (Test)|0.6774|0.6746|-0.2724|-0.0313|0.6774|0.6512|-0.2494|0.0578|\n",
    "\n",
    "#### Optimal Parameters\n",
    "\n",
    "| Dataset |Sex (Class. thresh.)|Sex (Class. thresh. - fairness)|Sex (ROC margin - fairness)| Race/Age (Class. thresh.)|Race/Age (Class. thresh. - fairness)|Race/Age (ROC margin - fairness)|\n",
    "|-|-|-|-|-|-|-|\n",
    "|Adult|0.2674|0.5049|0.1819|0.2674|0.5049|0.0808|\n",
    "|German|0.6732|0.6237|0.0538|0.6732|0.7029|0.0728|\n",
    "|Compas|0.5148|0.5841|0.0679|0.5148|0.5841|0.0679|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metric: Average odds difference, Accuracy Metric: Balanced accuracy\n",
    "\n",
    "#### Performance\n",
    "\n",
    "| Dataset |Sex (Acc-Bef)|Sex (Acc-Aft)|Sex (Fair-Bef)|Sex (Fair-Aft)|Race/Age (Acc-Bef)|Race/Age (Acc-Aft)|Race/Age (Fair-Bef)|Race/Age (Fair-Aft)|\n",
    "|-|-|-|-|-|-|-|-|-|\n",
    "|Adult (Valid)|0.7473|0.6058|-0.2910|-0.0385|0.7473|0.6593|-0.1947|-0.0444|\n",
    "|Adult (Test)|0.7417|0.6024|-0.3281|-0.0438|0.7417|0.6611|-0.1991|-0.0121|\n",
    "|German (Valid)|0.6930|0.6930|-0.0039|-0.0039|0.6930|0.6807|-0.0919|-0.0193|\n",
    "|German (Test)|0.6524|0.6571|0.0071|0.0237|0.6524|0.6587|-0.3278|-0.2708|\n",
    "|Compas (Valid)|0.6599|0.6416|-0.2285|-0.0332|0.6599|0.6646|-0.2918|-0.0105|\n",
    "|Compas (Test)|0.6774|0.6721|-0.2439|-0.0716|0.6774|0.6512|-0.1927|0.1145|\n",
    "\n",
    "#### Optimal Parameters\n",
    "\n",
    "| Dataset |Sex (Class. thresh.)|Sex (Class. thresh. - fairness)|Sex (ROC margin - fairness)| Race/Age (Class. thresh.)|Race/Age (Class. thresh. - fairness)|Race/Age (ROC margin - fairness)|\n",
    "|-|-|-|-|-|-|-|\n",
    "|Adult|0.2674|0.5049|0.1212|0.2674|0.5049|0.0505|\n",
    "|German|0.6732|0.6633|0.0137|0.6732|0.6732|0.0467|\n",
    "|Compas|0.5148|0.5742|0.0608|0.5148|0.5841|0.0679|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metric: Equal opportunity difference, Accuracy Metric: Balanced accuracy\n",
    "\n",
    "#### Performance\n",
    "\n",
    "| Dataset |Sex (Acc-Bef)|Sex (Acc-Aft)|Sex (Fair-Bef)|Sex (Fair-Aft)|Race/Age (Acc-Bef)|Race/Age (Acc-Aft)|Race/Age (Fair-Bef)|Race/Age (Fair-Aft)|\n",
    "|-|-|-|-|-|-|-|-|-|\n",
    "|Adult (Valid)|0.7473|0.6051|-0.3066|-0.0136|0.7473|0.6198|-0.2285|0.0287|\n",
    "|Adult (Test)|0.7417|0.5968|-0.4001|-0.0415|0.7417|0.6202|-0.2165|0.1193|\n",
    "|German (Valid)|0.6930|0.6930|-0.0347|-0.0347|0.6930|0.6597|0.1162|-0.0210|\n",
    "|German (Test)|0.6524|0.6571|0.0400|0.0733|0.6524|0.6190|-0.3556|-0.4333|\n",
    "|Compas (Valid)|0.6599|0.6416|-0.1938|0.0244|0.6599|0.6646|-0.2315|0.0002|\n",
    "|Compas (Test)|0.6774|0.6721|-0.1392|0.0236|0.6774|0.6512|-0.1877|0.1196|\n",
    "\n",
    "#### Optimal Parameters\n",
    "\n",
    "| Dataset |Sex (Class. thresh.)|Sex (Class. thresh. - fairness)|Sex (ROC margin - fairness)| Race/Age (Class. thresh.)|Race/Age (Class. thresh. - fairness)|Race/Age (ROC margin - fairness)|\n",
    "|-|-|-|-|-|-|-|\n",
    "|Adult|0.2674|0.5049|0.1819|0.2674|0.5049|0.0808|\n",
    "|German|0.6732|0.6633|0.0137|0.6732|0.6039|0.0000|\n",
    "|Compas|0.5148|0.5742|0.0608|0.5148|0.5841|0.0679|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: AIF360 Reject Option Classification (FairFace Dataset)\n",
    "Source: https://medium.com/@james.irving.phd/blog-post-series-ai-fairness-360-mitigating-bias-in-machine-learning-models-c1ec744c91c4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply FairFace Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare FairFace dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'gender', 'race', 'service_test']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the test set with the FairFace test set\n",
    "\n",
    "df_train = pd.read_csv('fairface_label_train.csv')\n",
    "df_train['split'] = 'train'\n",
    "df_test = pd.read_csv('fairface_label_val.csv')\n",
    "df_test['split'] = 'test'\n",
    "df = pd.concat([df_train, df_test])\n",
    "\n",
    "df = pd.read_csv('fairface_label_val.csv')\n",
    "df = df.drop(columns=['file'])\n",
    "df.columns.values.tolist()  # to see all the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'Sex', 'Race', 'service_test']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'gender':'Sex'}, inplace=True)\n",
    "df.rename(columns={'race':'Race'}, inplace=True)\n",
    "\n",
    "df.columns.values.tolist()  # to see all the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'Sex', 'Race', 'service_test'], dtype='object')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter service_test == True\n",
    "\n",
    "df = df[df['service_test'] == True]\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0], dtype=int64), array([1, 0], dtype=int64))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the 'race' column as binary white or non-white.\n",
    "Race_map = {'East Asian': 1, 'White': 0, 'Latino_Hispanic': 1,\n",
    "            'Southeast Asian': 1, 'Black': 1, 'Indian': 1, 'Middle Eastern': 1}\n",
    "df['Race'] = df['Race'].map(Race_map)\n",
    "\n",
    "# Encode the \"gender\" column\n",
    "Sex_map = {\"Male\": 0, \"Female\":1}\n",
    "df['Sex'] = df['Sex'].map(Sex_map)\n",
    "\n",
    "\n",
    "df['Race'].unique(), df['Sex'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the protcted attribute mapping dictionaries to a dict\n",
    "protected_attribute_maps = {\"Race\": Race_map,\n",
    "                            \"Sex\": Sex_map}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute NaNs with most frequent value\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# Enable pandas dataframe output\n",
    "from sklearn import set_config\n",
    "set_config(transform_output='pandas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;onehotencoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 Index([&#x27;age&#x27;], dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n",
       "                                                  SimpleImputer())]),\n",
       "                                 Index([&#x27;Sex&#x27;, &#x27;Race&#x27;], dtype=&#x27;object&#x27;))],\n",
       "                  verbose_feature_names_out=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>ColumnTransformer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for ColumnTransformer</span></a><span class=\"sk-estimator-doc-link \">i<span>Not fitted</span></span></div></label><div class=\"sk-toggleable__content \"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;onehotencoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 Index([&#x27;age&#x27;], dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;simpleimputer&#x27;,\n",
       "                                                  SimpleImputer())]),\n",
       "                                 Index([&#x27;Sex&#x27;, &#x27;Race&#x27;], dtype=&#x27;object&#x27;))],\n",
       "                  verbose_feature_names_out=False)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>cat</div></div></label><div class=\"sk-toggleable__content \"><pre>Index([&#x27;age&#x27;], dtype=&#x27;object&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>SimpleImputer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></div></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>OneHotEncoder</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a></div></label><div class=\"sk-toggleable__content \"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>num</div></div></label><div class=\"sk-toggleable__content \"><pre>Index([&#x27;Sex&#x27;, &#x27;Race&#x27;], dtype=&#x27;object&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>SimpleImputer</div></div><div><a class=\"sk-estimator-doc-link \" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.impute.SimpleImputer.html\">?<span>Documentation for SimpleImputer</span></a></div></label><div class=\"sk-toggleable__content \"><pre>SimpleImputer()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>remainder</div></div></label><div class=\"sk-toggleable__content \"><pre></pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator  sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label  sk-toggleable__label-arrow\"><div><div>passthrough</div></div></label><div class=\"sk-toggleable__content \"><pre>passthrough</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(remainder='passthrough',\n",
       "                  transformers=[('cat',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse_output=False))]),\n",
       "                                 Index(['age'], dtype='object')),\n",
       "                                ('num',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer())]),\n",
       "                                 Index(['Sex', 'Race'], dtype='object'))],\n",
       "                  verbose_feature_names_out=False)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical Pipeline\n",
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "cat_pipe = make_pipeline(cat_imputer, ohe)\n",
    "\n",
    "# Numeric Pipeline\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "num_pipe = make_pipeline(num_imputer)\n",
    "\n",
    "# Convert Boolean Columns to Integers\n",
    "bool_cols = df.select_dtypes(include='bool').columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "\n",
    "# Create the column Transformer\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', cat_pipe, cat_cols),\n",
    "                                               ('num', num_pipe, num_cols)],\n",
    "                                 remainder='passthrough',\n",
    "                                 verbose_feature_names_out=False)\n",
    "\n",
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age_0-2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_10-19",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_20-29",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_3-9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_30-39",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_40-49",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_50-59",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_60-69",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_more than 70",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sex",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Race",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "service_test",
         "rawType": "int32",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6a3f330c-f3fc-4c0e-bbb5-16211f71b950",
       "rows": [
        [
         "1",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1"
        ],
        [
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1"
        ],
        [
         "3",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1"
        ],
        [
         "6",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1"
        ],
        [
         "10",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "1"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_0-2</th>\n",
       "      <th>age_10-19</th>\n",
       "      <th>age_20-29</th>\n",
       "      <th>age_3-9</th>\n",
       "      <th>age_30-39</th>\n",
       "      <th>age_40-49</th>\n",
       "      <th>age_50-59</th>\n",
       "      <th>age_60-69</th>\n",
       "      <th>age_more than 70</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>service_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age_0-2  age_10-19  age_20-29  age_3-9  age_30-39  age_40-49  age_50-59  \\\n",
       "1       0.0        0.0        0.0      0.0        0.0        0.0        1.0   \n",
       "2       0.0        0.0        0.0      0.0        1.0        0.0        0.0   \n",
       "3       0.0        0.0        1.0      0.0        0.0        0.0        0.0   \n",
       "6       0.0        0.0        1.0      0.0        0.0        0.0        0.0   \n",
       "10      0.0        0.0        0.0      0.0        0.0        0.0        0.0   \n",
       "\n",
       "    age_60-69  age_more than 70  Sex  Race  service_test  \n",
       "1         0.0               0.0  1.0   1.0             1  \n",
       "2         0.0               0.0  0.0   0.0             1  \n",
       "3         0.0               0.0  1.0   1.0             1  \n",
       "6         0.0               0.0  0.0   1.0             1  \n",
       "10        0.0               1.0  1.0   1.0             1  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and Transform the data\n",
    "final_df = preprocessor.fit_transform(df)\n",
    "final_df.head()  # Display the first few rows of the transformed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- protected_attribute_maps:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Race': {'East Asian': 1,\n",
       "  'White': 0,\n",
       "  'Latino_Hispanic': 1,\n",
       "  'Southeast Asian': 1,\n",
       "  'Black': 1,\n",
       "  'Indian': 1,\n",
       "  'Middle Eastern': 1},\n",
       " 'Sex': {'Male': 0, 'Female': 1}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- prot_attrs_priv_group_names:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Race': ['White'], 'Sex': ['Male']}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Saving the protcted attribute mapping dictionaries to a dict\n",
    "\n",
    "#Commented Out: previously saved above\n",
    "# protected_attribute_maps = {\"Race\":race_map,\n",
    "#                             \"Sex\":sex_map}\n",
    "\n",
    "print(\"- protected_attribute_maps:\")\n",
    "display(protected_attribute_maps)\n",
    "\n",
    "# Manually defining the list of privileged group NAMES to a dict\n",
    "prot_attrs_priv_group_names = {'Race':[\"White\"],\n",
    "                               \"Sex\":[\"Male\"]}\n",
    "\n",
    "print(\"- prot_attrs_priv_group_names:\")\n",
    "prot_attrs_priv_group_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['feature_names', 'label_names', 'protected_attributes_names', 'privileged_protected_attributes', 'unprivileged_protected_attributes'])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a data dictionary like the one used in AIF360's medical expenditure example\n",
    "target = 'East Asian'  # The target variable for classification\n",
    "\n",
    "# Ensure the target column exists in final_df before dropping it\n",
    "if target not in final_df.columns:\n",
    "    final_df[target] = np.random.choice([0, 1], size=len(final_df))  # Add the target column if missing\n",
    "\n",
    "# Saving the feature names as a list\n",
    "feature_names = final_df.drop(columns=target).columns\n",
    "protected_attributes_names = list(protected_attribute_maps.keys())\n",
    "\n",
    "# Starting our own data dictionary\n",
    "DATA_DICT = dict(feature_names=feature_names, # list of feature columns\n",
    "                 label_names = [target], # list with just the target label column\n",
    "                 protected_attributes_names = protected_attributes_names, # list of protected attribute columns\n",
    "                 \n",
    "                # Saving information about each row (not used )\n",
    "                #  instance_names = final_df.index, # list of each row's name\n",
    "                #  instance_weights = np.ones_like(final_df.index), # list of each row's weight (1.0 for now\n",
    "                \n",
    "                \n",
    "                # List of arrays of privileged group numbers: will be filled in later\n",
    "                privileged_protected_attributes = [], \n",
    "                # List of arrays of unprivileged group numers: Will be filled in later\n",
    "                unprivileged_protected_attributes = [], \n",
    ")\n",
    "                 \n",
    "DATA_DICT.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'East Asian': 1,\n",
       " 'White': 0,\n",
       " 'Latino_Hispanic': 1,\n",
       " 'Southeast Asian': 1,\n",
       " 'Black': 1,\n",
       " 'Indian': 1,\n",
       " 'Middle Eastern': 1}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numeric index of current protected attribute\n",
    "attr_idx = 0\n",
    "\n",
    "# slice the name of the current protected attribute\n",
    "attr_name = protected_attributes_names[attr_idx]\n",
    "\n",
    "# Get the mapping for the current protected attribute\n",
    "attr_map = protected_attribute_maps[attr_name]\n",
    "attr_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Get the privileged group names for the current protected attribute\n",
    "priv_group_names = prot_attrs_priv_group_names[attr_name]\n",
    "priv_group_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the privileged values for the current protected attribute\n",
    "privileged_group_nums = np.array([attr_map[pg] for pg in priv_group_names])\n",
    "privileged_group_nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unprivileged values for the current protected attribute\n",
    "unprivileged_group_nums = np.array(\n",
    "    [v for v in attr_map.values() if v not in privileged_group_nums])\n",
    "unprivileged_group_nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the privileged and unprivileged group values to the data dictionary\n",
    "DATA_DICT['privileged_protected_attributes'].append(privileged_group_nums)\n",
    "DATA_DICT['unprivileged_protected_attributes'].append(unprivileged_group_nums)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the dataframe to AIF360 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['feature_names', 'label_names', 'protected_attributes_names', 'privileged_protected_attributes', 'unprivileged_protected_attributes'])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviewing the structure of our DATA_DICT\n",
    "DATA_DICT.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                              \\\n",
       "                                                                       \n",
       "                                 age_0-2 age_10-19 age_20-29 age_3-9   \n",
       "instance names                                                         \n",
       "1                           1.0      0.0       0.0       0.0     0.0   \n",
       "2                           1.0      0.0       0.0       0.0     0.0   \n",
       "3                           1.0      0.0       0.0       1.0     0.0   \n",
       "6                           1.0      0.0       0.0       1.0     0.0   \n",
       "10                          1.0      0.0       0.0       0.0     0.0   \n",
       "...                         ...      ...       ...       ...     ...   \n",
       "10939                       1.0      0.0       0.0       1.0     0.0   \n",
       "10941                       1.0      0.0       0.0       0.0     1.0   \n",
       "10947                       1.0      0.0       1.0       0.0     0.0   \n",
       "10949                       1.0      0.0       0.0       0.0     0.0   \n",
       "10953                       1.0      0.0       0.0       0.0     0.0   \n",
       "\n",
       "                                                                         \\\n",
       "                                                                          \n",
       "               age_30-39 age_40-49 age_50-59 age_60-69 age_more than 70   \n",
       "instance names                                                            \n",
       "1                    0.0       0.0       1.0       0.0              0.0   \n",
       "2                    1.0       0.0       0.0       0.0              0.0   \n",
       "3                    0.0       0.0       0.0       0.0              0.0   \n",
       "6                    0.0       0.0       0.0       0.0              0.0   \n",
       "10                   0.0       0.0       0.0       0.0              1.0   \n",
       "...                  ...       ...       ...       ...              ...   \n",
       "10939                0.0       0.0       0.0       0.0              0.0   \n",
       "10941                0.0       0.0       0.0       0.0              0.0   \n",
       "10947                0.0       0.0       0.0       0.0              0.0   \n",
       "10949                1.0       0.0       0.0       0.0              0.0   \n",
       "10953                0.0       1.0       0.0       0.0              0.0   \n",
       "\n",
       "                                                     labels  \n",
       "               protected attribute                           \n",
       "                               Sex Race service_test         \n",
       "instance names                                               \n",
       "1                              1.0  1.0          1.0    0.0  \n",
       "2                              0.0  0.0          1.0    1.0  \n",
       "3                              1.0  1.0          1.0    0.0  \n",
       "6                              0.0  1.0          1.0    1.0  \n",
       "10                             1.0  1.0          1.0    0.0  \n",
       "...                            ...  ...          ...    ...  \n",
       "10939                          1.0  1.0          1.0    0.0  \n",
       "10941                          0.0  1.0          1.0    0.0  \n",
       "10947                          0.0  0.0          1.0    0.0  \n",
       "10949                          0.0  0.0          1.0    0.0  \n",
       "10953                          0.0  1.0          1.0    1.0  \n",
       "\n",
       "[5162 rows x 14 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "# Add the target column 'Reincarcerated' to final_df\n",
    "# For demonstration, we assign random binary values (0 or 1) as the target.\n",
    "# Replace this logic with your actual target column values.\n",
    "#final_df['Reincarcerated'] = np.random.choice([0, 1], size=len(final_df))\n",
    "\n",
    "# Update the BinaryLabelDataset creation\n",
    "binary_dataset = BinaryLabelDataset(df=final_df,\n",
    "                                    label_names=DATA_DICT['label_names'],\n",
    "                                    protected_attribute_names=DATA_DICT['protected_attributes_names'],\n",
    "                                    favorable_label=0,  # Non-recidivism is favorable\n",
    "                                    unfavorable_label=1  # Recidivism is unfavorable\n",
    "                                    )\n",
    "binary_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DICT['unprivileged_protected_attributes'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Race', 'Sex'], [array([0])], [array([1, 1, 1, 1, 1, 1])])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DICT['protected_attributes_names'], DATA_DICT['privileged_protected_attributes'], DATA_DICT['unprivileged_protected_attributes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'Race': array([1])}], [{'Race': array([0])}])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEMP  testing of args - single dict per attribute\n",
    "\n",
    "# For loop (based on tutorial list comp) to create a list of dictionaries \n",
    "# for unprivileged and privileged groups for each of the protected attributes\n",
    "unprivileged_groups = []\n",
    "privileged_groups = []\n",
    "\n",
    "# Ensure the loop does not exceed the length of DATA_DICT['unprivileged_protected_attributes']\n",
    "num_attributes = min(len(protected_attributes_names), len(DATA_DICT['unprivileged_protected_attributes']))\n",
    "num_attributes = min(len(protected_attributes_names), len(\n",
    "    DATA_DICT['privileged_protected_attributes']))\n",
    "\n",
    "for sens_ind in range(num_attributes):\n",
    "    # Saving the name of the current protected attribute\n",
    "    sens_attr_name = protected_attributes_names[sens_ind]\n",
    "    \n",
    "    # Save the unique integer values for the current protected attribute\n",
    "    unprivileged_groups.append({sens_attr_name: np.unique(DATA_DICT['unprivileged_protected_attributes'][sens_ind])})\n",
    "    privileged_groups.append({sens_attr_name:  np.unique(DATA_DICT['privileged_protected_attributes'][sens_ind])})\n",
    "\n",
    "unprivileged_groups, privileged_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BinaryLabelDatasetMetric class must be instantiated for 1 protected attribute at a time (e.g. Sex or Race, but not both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.explainers import Explainer\n",
    "from aif360.metrics import Metric\n",
    "\n",
    "\n",
    "class MetricTextExplainer(Explainer):\n",
    "    \"\"\"Class for explaining metric values with text.\n",
    "\n",
    "    These briefly explain what a metric is and/or how it is calculated unless it\n",
    "    is obvious (e.g. accuracy) and print the value.\n",
    "\n",
    "    This class contains text explanations for all metric values regardless of\n",
    "    which subclass they appear in. This will raise an error if the metric does\n",
    "    not apply (e.g. calling `true_positive_rate` if\n",
    "    `type(metric) == DatasetMetric`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, metric):\n",
    "        \"\"\"Initialize a `MetricExplainer` object.\n",
    "\n",
    "        Args:\n",
    "            metric (Metric): The metric to be explained.\n",
    "        \"\"\"\n",
    "        if isinstance(metric, Metric):\n",
    "            self.metric = metric\n",
    "        else:\n",
    "            raise TypeError(\"metric must be a Metric.\")\n",
    "\n",
    "    def accuracy(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Classification accuracy (ACC): {}\".format(\n",
    "                self.metric.accuracy(privileged=privileged))\n",
    "        return \"Classification accuracy on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.accuracy(privileged=privileged))\n",
    "\n",
    "    def average_abs_odds_difference(self):\n",
    "        return (\"Average absolute odds difference (average of abs(TPR \"\n",
    "                \"difference) and abs(FPR difference)): {}\".format(\n",
    "                    self.metric.average_abs_odds_difference()))\n",
    "\n",
    "    def average_odds_difference(self):\n",
    "        return (\"Average odds difference (average of TPR difference and FPR \"\n",
    "                \"difference, 0 = equality of odds): {}\".format(\n",
    "                    self.metric.average_odds_difference()))\n",
    "\n",
    "    def between_all_groups_coefficient_of_variation(self):\n",
    "        return \"Between-group coefficient of variation: {}\".format(\n",
    "            self.metric.between_all_groups_coefficient_of_variation())\n",
    "\n",
    "    def between_all_groups_generalized_entropy_index(self, alpha=2):\n",
    "        return \"Between-group generalized entropy index: {}\".format(\n",
    "            self.metric.between_all_groups_generalized_entropy_index(alpha=alpha))\n",
    "\n",
    "    def between_all_groups_theil_index(self):\n",
    "        return \"Between-group Theil index: {}\".format(\n",
    "            self.metric.between_all_groups_theil_index())\n",
    "\n",
    "    def between_group_coefficient_of_variation(self):\n",
    "        return \"Between-group coefficient of variation: {}\".format(\n",
    "            self.metric.between_group_coefficient_of_variation())\n",
    "\n",
    "    def between_group_generalized_entropy_index(self, alpha=2):\n",
    "        return \"Between-group generalized entropy index: {}\".format(\n",
    "            self.metric.between_group_generalized_entropy_index(alpha=alpha))\n",
    "\n",
    "    def between_group_theil_index(self):\n",
    "        return \"Between-group Theil index: {}\".format(\n",
    "            self.metric.between_group_theil_index())\n",
    "\n",
    "    def coefficient_of_variation(self):\n",
    "        return \"Coefficient of variation: {}\".format(\n",
    "            self.metric.coefficient_of_variation())\n",
    "\n",
    "    def consistency(self, n_neighbors=5):\n",
    "        return \"Consistency (Zemel, et al. 2013): {}\".format(\n",
    "            self.metric.consistency(n_neighbors=n_neighbors))\n",
    "\n",
    "    def disparate_impact(self):\n",
    "        return (\"Disparate impact (probability of favorable outcome for \"\n",
    "                \"unprivileged instances / probability of favorable outcome for \"\n",
    "                \"privileged instances): {}\".format(\n",
    "                    self.metric.disparate_impact()))\n",
    "\n",
    "    def error_rate(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Error rate (ERR = 1 - ACC): {}\".format(\n",
    "                self.metric.error_rate(privileged=privileged))\n",
    "        return \"Error rate on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.error_rate(privileged))\n",
    "\n",
    "    def error_rate_difference(self):\n",
    "        return (\"Error rate difference (error rate on unprivileged instances - \"\n",
    "                \"error rate on privileged instances): {}\".format(\n",
    "                    self.metric.error_rate_difference()))\n",
    "\n",
    "    def error_rate_ratio(self):\n",
    "        return (\"Error rate ratio (error rate on unprivileged instances / \"\n",
    "                \"error rate on privileged instances): {}\".format(\n",
    "                    self.metric.error_rate_ratio()))\n",
    "\n",
    "    def false_discovery_rate(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"False discovery rate (FDR = FP / (FP + TP)): {}\".format(\n",
    "                self.metric.false_discovery_rate(privileged=privileged))\n",
    "        return \"False discovery rate on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.false_discovery_rate(privileged=privileged))\n",
    "\n",
    "    def false_discovery_rate_difference(self):\n",
    "        return (\"False discovery rate difference (false discovery rate on \"\n",
    "                \"unprivileged instances - false discovery rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.false_discovery_rate_difference()))\n",
    "\n",
    "    def false_discovery_rate_ratio(self):\n",
    "        return (\"False discovery rate ratio (false discovery rate on \"\n",
    "                \"unprivileged instances - false discovery rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.false_discovery_rate_ratio()))\n",
    "\n",
    "    def false_negative_rate(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"False negative rate (FNR = FN / (TP + FN)): {}\".format(\n",
    "                self.metric.false_negative_rate(privileged=privileged))\n",
    "        return \"False negative rate on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.false_negative_rate(privileged=privileged))\n",
    "\n",
    "    def false_negative_rate_difference(self):\n",
    "        return (\"False negative rate difference (false negative rate on \"\n",
    "                \"unprivileged instances - false negative rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.false_negative_rate_difference()))\n",
    "\n",
    "    def false_negative_rate_ratio(self):\n",
    "        return (\"False negative rate ratio (false negative rate on \"\n",
    "                \"unprivileged instances / false negative rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.false_negative_rate_ratio()))\n",
    "\n",
    "    def false_omission_rate(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"False omission rate (FOR = FN / (FN + TN)): {}\".format(\n",
    "                self.metric.false_omission_rate(privileged=privileged))\n",
    "        return \"False omission rate on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.false_omission_rate(privileged=privileged))\n",
    "\n",
    "    def falses_omission_rate_difference(self):\n",
    "        return (\"False omission rate difference (falses omission rate on \"\n",
    "                \"unprivileged instances - falses omission rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.falses_omission_rate_difference()))\n",
    "\n",
    "    def false_omission_rate_ratio(self):\n",
    "        return (\"False omission rate ratio (false omission rate on \"\n",
    "                \"unprivileged instances - false omission rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.false_omission_rate_ratio()))\n",
    "\n",
    "    def false_positive_rate(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"False positive rate (FPR = FP / (FP + TN)): {}\".format(\n",
    "                self.metric.false_positive_rate(privileged=privileged))\n",
    "        return \"False positive rate on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.false_positive_rate(privileged=privileged))\n",
    "\n",
    "    def false_positive_rate_difference(self):\n",
    "        return (\"False positive rate difference (false positive rate on \"\n",
    "                \"unprivileged instances - false positive rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.false_positive_rate_difference()))\n",
    "\n",
    "    def false_positive_rate_ratio(self):\n",
    "        return (\"False positive rate ratio (false positive rate on \"\n",
    "                \"unprivileged instances / false positive rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.false_positive_rate_ratio()))\n",
    "\n",
    "    def generalized_entropy_index(self, alpha=2):\n",
    "        return \"Generalized entropy index (GE(alpha)): {}\".format(\n",
    "            self.metric.generalized_entropy_index(alpha=alpha))\n",
    "\n",
    "    def mean_difference(self):\n",
    "        return (\"Mean difference (mean label value on unprivileged instances - \"\n",
    "                \"mean label value on privileged instances): {}\".format(\n",
    "                    self.metric.mean_difference()))\n",
    "\n",
    "    def negative_predictive_value(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Negative predictive value (NPV = TN / (TN + FN)): {}\".format(\n",
    "                self.metric.negative_predictive_value(privileged=privileged))\n",
    "        return \"Negative predictive value on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.negative_predictive_value(privileged=privileged))\n",
    "\n",
    "    def num_false_negatives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of false negative instances (FN): {}\".format(\n",
    "                self.metric.num_false_negatives(privileged=privileged))\n",
    "        return \"Number of {} false negative instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_false_negatives(privileged=privileged))\n",
    "\n",
    "    def num_false_positives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of false positive instances (FP): {}\".format(\n",
    "                self.metric.num_false_positives(privileged=privileged))\n",
    "        return \"Number of {} false positive instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_false_positives(privileged=privileged))\n",
    "\n",
    "    def num_instances(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of instances: {}\".format(\n",
    "                self.metric.num_instances(privileged=privileged))\n",
    "        return \"Number of {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_instances(privileged=privileged))\n",
    "\n",
    "    def num_negatives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of negative-outcome instances: {}\".format(\n",
    "                self.metric.num_negatives(privileged=privileged))\n",
    "        return \"Number of {} negative-outcome instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_negatives(privileged=privileged))\n",
    "\n",
    "    def num_positives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of positive-outcome instances: {}\".format(\n",
    "                self.metric.num_positives(privileged=privileged))\n",
    "        return \"Number of {} positive-outcome instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_positives(privileged=privileged))\n",
    "\n",
    "    def num_pred_negatives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of negative-outcome instances predicted: {}\".format(\n",
    "                self.metric.num_pred_negatives(privileged=privileged))\n",
    "        return \"Number of {} negative-outcome instances predicted: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_pred_negatives(privileged=privileged))\n",
    "\n",
    "    def num_pred_positives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of positive-outcome instances predicted: {}\".format(\n",
    "                self.metric.num_pred_positives(privileged=privileged))\n",
    "        return \"Number of {} positive-outcome instances predicted: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_pred_positives(privileged=privileged))\n",
    "\n",
    "    def num_true_negatives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of true negative instances (TN): {}\".format(\n",
    "                self.metric.num_true_negatives(privileged=privileged))\n",
    "        return \"Number of {} true negative instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_true_negatives(privileged=privileged))\n",
    "\n",
    "    def num_true_positives(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Number of true positive instances (TP): {}\".format(\n",
    "                self.metric.num_true_positives(privileged=privileged))\n",
    "        return \"Number of {} true positive instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.num_true_positives(privileged=privileged))\n",
    "\n",
    "    def positive_predictive_value(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"Positive predictive value (PPV, precision = TP / (TP + FP)): {}\".format(\n",
    "                self.metric.positive_predictive_value(privileged=privileged))\n",
    "        return \"Positive predictive value on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.positive_predictive_value(privileged=privileged))\n",
    "\n",
    "    def statistical_parity_difference(self):\n",
    "        return (\"Statistical parity difference (probability of favorable \"\n",
    "                \"outcome for unprivileged instances - probability of favorable \"\n",
    "                \"outcome for privileged instances): {}\".format(\n",
    "                    self.metric.statistical_parity_difference()))\n",
    "\n",
    "    def theil_index(self):\n",
    "        return \"Theil index (generalized entropy index with alpha = 1): {}\".format(\n",
    "            self.metric.theil_index())\n",
    "\n",
    "    def true_negative_rate(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"True negative rate (TNR, specificity = TN / (FP + TN)): {}\".format(\n",
    "                self.metric.true_negative_rate(privileged=privileged))\n",
    "        return \"True negative rate on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.true_negative_rate(privileged=privileged))\n",
    "\n",
    "    def true_positive_rate(self, privileged=None):\n",
    "        if privileged is None:\n",
    "            return \"True positive rate (TPR, recall, sensitivity = TP / (TP + FN)): {}\".format(\n",
    "                self.metric.true_positive_rate(privileged=privileged))\n",
    "        return \"True positive rate on {} instances: {}\".format(\n",
    "            'privileged' if privileged else 'unprivileged',\n",
    "            self.metric.true_positive_rate(privileged=privileged))\n",
    "\n",
    "    def true_positive_rate_difference(self):\n",
    "        return (\"True positive rate difference (true positive rate on \"\n",
    "                \"unprivileged instances - true positive rate on privileged \"\n",
    "                \"instances): {}\".format(\n",
    "                    self.metric.true_positive_rate_difference()))\n",
    "\n",
    "    # ============================== ALIASES ===================================\n",
    "    def equal_opportunity_difference(self):\n",
    "        return self.true_positive_rate_difference()\n",
    "\n",
    "    def power(self, privileged=None):\n",
    "        return self.num_true_positives(privileged=privileged)\n",
    "\n",
    "    def precision(self, privileged=None):\n",
    "        return self.positive_predictive_value(privileged=privileged)\n",
    "\n",
    "    def recall(self, privileged=None):\n",
    "        return self.true_positive_rate(privileged=privileged)\n",
    "\n",
    "    def sensitivity(self, privileged=None):\n",
    "        return self.true_positive_rate(privileged=privileged)\n",
    "\n",
    "    def specificity(self, privileged=None):\n",
    "        return self.true_negative_rate(privileged=privileged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<aif360.metrics.binary_label_dataset_metric.BinaryLabelDatasetMetric at 0x2950ecd1450>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Calculating the disparate impact for Race\n",
    "sens_ind = 0\n",
    "\n",
    "# Printing the name of the sensitive attribute\n",
    "print(f\"{list(unprivileged_groups[sens_ind].keys())[0]}\")\n",
    "\n",
    "metric_race = BinaryLabelDatasetMetric(binary_dataset,\n",
    "                                       privileged_groups=[\n",
    "                                           privileged_groups[sens_ind]],\n",
    "                                       unprivileged_groups=[\n",
    "                                           unprivileged_groups[sens_ind]]\n",
    "                                       )\n",
    "metric_race\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MetricTextExplainer at 0x2950ead2190>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a metric text explainer using the BinaryLabelDatasetMetric object\n",
    "explainer_race = MetricTextExplainer(metric_race)\n",
    "explainer_race\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MetricTextExplainer at 0x2950f1e0c90>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the disparate impact for the available protected attribute and instantiate the explainer\n",
    "sens_ind = 0  # Use 0 since only one attribute is present in the groups lists\n",
    "metric_sex = BinaryLabelDatasetMetric(\n",
    "\tbinary_dataset,\n",
    "\tprivileged_groups=[privileged_groups[sens_ind]],\n",
    "\tunprivileged_groups=[unprivileged_groups[sens_ind]]\n",
    ")\n",
    "explainer_sex = MetricTextExplainer(metric_sex)\n",
    "explainer_sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race\n"
     ]
    }
   ],
   "source": [
    "# Calculating the disparate impact for sex\n",
    "sens_ind = 0  # Ensure this index is within the range of unprivileged_groups and privileged_groups\n",
    "print(f\"{list(unprivileged_groups[sens_ind].keys())[0]}\")\n",
    "\n",
    "# Instantiate the metric class for the protected attribute.\n",
    "metric_sex = BinaryLabelDatasetMetric(binary_dataset,\n",
    "                                  privileged_groups=[privileged_groups[sens_ind]],\n",
    "                                  unprivileged_groups=[unprivileged_groups[sens_ind]]\n",
    "                                  )\n",
    "\n",
    "# Instantiate the explainer for each protected attribute\n",
    "explainer_sex = MetricTextExplainer(metric_sex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics\n",
    "\n",
    "- Disparate Impact\n",
    "- Mean Difference\n",
    "- Base Rate\n",
    "- Statistical Parity Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparate Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact (Sex): 1.0213159658478306\n",
      "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 1.0213159658478306\n"
     ]
    }
   ],
   "source": [
    "# Calculate the disparate impact for sex\n",
    "print(f\"Disparate Impact (Sex): {metric_sex.disparate_impact()}\")\n",
    "\n",
    "# Exlpaining the disparate impact\n",
    "print(explainer_sex.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Sex, since the value is greater than 1 (1.04) the dataset is biased, with Females being more likely to be favorable than male. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact (Race): 1.0213159658478306\n",
      "Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 1.0213159658478306\n"
     ]
    }
   ],
   "source": [
    "# Display the disparate impact for Race\n",
    "print(f\"Disparate Impact (Race): {metric_race.disparate_impact()}\")\n",
    "print(explainer_race.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of Race, since the value is greater than 1 (1.04) the dataset is actually biased with non-whites being more likely to be favorable than male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Difference (Sex): 0.010803187050944074\n",
      "Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.010803187050944074\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Difference (Sex): {metric_sex.mean_difference()}\")\n",
    "print(explainer_sex.mean_difference())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Difference (Race): 0.010803187050944074\n",
      "Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 0.010803187050944074\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Difference (Race): {metric_race.mean_difference()}\")\n",
    "print(explainer_race.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Rate\n",
    "# print(explainer_train.base_rate())\n",
    "priv_base_rate = metric_sex.base_rate()\n",
    "unprov_base_rate = metric_sex.base_rate(privileged=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For protected attribute: Sex\n",
      "- Privileged Base Rate: 0.5160790391321194\n",
      "- Unprivileged Base Rate: 0.5176151761517616\n"
     ]
    }
   ],
   "source": [
    "print(\"For protected attribute: Sex\")\n",
    "print(f\"- Privileged Base Rate: {priv_base_rate}\")\n",
    "print(f\"- Unprivileged Base Rate: {unprov_base_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Rate\n",
    "# print(explainer_train.base_rate())\n",
    "priv_base_rate = metric_race.base_rate()\n",
    "unprov_base_rate = metric_race.base_rate(privileged=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For protected attribute: Race\n",
      "- Privileged Base Rate: 0.5160790391321194\n",
      "- Unprivileged Base Rate: 0.5176151761517616\n"
     ]
    }
   ],
   "source": [
    "print(\"For protected attribute: Race\")\n",
    "print(f\"- Privileged Base Rate: {priv_base_rate}\")\n",
    "print(f\"- Unprivileged Base Rate: {unprov_base_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Parity Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (Sex): 0.010803187050944074\n"
     ]
    }
   ],
   "source": [
    "# Statistical Parity Difference for Sex\n",
    "print(\n",
    "    f\"Statistical Parity Difference (Sex): {metric_sex.statistical_parity_difference()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (Race): 0.010803187050944074\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Statistical Parity Difference (Race): {metric_race.statistical_parity_difference()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_orig_valid, dataset_orig_test = train_test_split(final_df, test_size=0.5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain scores from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "# X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "# y_test = dataset_orig_test_pred.labels\n",
    "# dataset_orig_test_pred.scores = lmod.predict_proba(X_test)[:, pos_ind].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7437\n",
      "Statistical parity difference = -0.3580\n",
      "Disparate impact = 0.2794\n",
      "Average odds difference = -0.3181\n",
      "Equal opportunity difference = -0.3769\n",
      "Theil index = 0.1129\n"
     ]
    }
   ],
   "source": [
    "# # Metrics for the test set\n",
    "# fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "# dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "# dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "# display(Markdown(\"#### Test set\"))\n",
    "# display(Markdown(\n",
    "#     \"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "# metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred,\n",
    "#                                   unprivileged_groups, privileged_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Transformed predictions - With fairness constraints"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.7141\n",
      "Statistical parity difference = -0.0402\n",
      "Disparate impact = 0.9088\n",
      "Average odds difference = 0.0423\n",
      "Equal opportunity difference = 0.0407\n",
      "Theil index = 0.1171\n"
     ]
    }
   ],
   "source": [
    "# # Metrics for the transformed test set\n",
    "# dataset_transf_test_pred = ROC.predict(dataset_orig_test_pred)\n",
    "\n",
    "# display(Markdown(\"#### Test set\"))\n",
    "# display(Markdown(\"##### Transformed predictions - With fairness constraints\"))\n",
    "# metric_test_aft = compute_metrics(dataset_orig_test, dataset_transf_test_pred,\n",
    "#                                   unprivileged_groups, privileged_groups)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
